<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="tYMwPoxNrWfLx2sDrDYzqI4-dq3M3FfI56NBL9JNtH8" />

    <title>4주차 ML 지도학습 두번째 시간</title>
    <meta name="description" content="A simple, whitespace, helvetica based portfolio theme.
">

    <link rel = "shortcut icon" type="image/x-icon" href="/img/square_logo.png">
    <link rel="stylesheet" href="/css/main.css">
<!--    <link rel="canonical" href="http://localhost:4000/posts/1st_term/ml/2021-10-06-ml_fourth_1_keonju/">-->
    <link rel="canonical" href="/posts/1st_term/ml/2021-10-06-ml_fourth_1_keonju/">

    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

    <!-- 카테고리 css -->
    <link rel="stylesheet" href="/css/pagination.css">
    <link href="/css/category.css" type="text/css" rel="stylesheet">
</head>

   
  <body>
    <script src="/js/default.js"></script>
    <header id="tab" >
  <script src="https://code.jquery.com/jquery-1.11.0.min.js"></script>
  <div class="site-header">
    <div class="wrapper">
      <div>
        <a href="/">
          <img src="/img/square_logo.png" align="left" width="57px">
        </a>
      </div>

      <nav class="site-nav">

        <div id="tab" class="trigger">
          <!-- GDSC Seoultech instead of blog -->
          <a class="page-link" href="/">GDSC</a>
          <a class="page-link" href="/members/2">MEMBER</a>
          <a class="page-link" href="/category/2nd_term">POST</a>
        </div>
      </nav>
    </div>
  </div>

  <div id="hovering" class="tab-header">
    <div class="wrapper">
      <div class="detail-post">
          <div class="tab-item">
            <div><a class="page-link-detail" href="/category/1st_term">1기</a></div>
            <div><a class="page-link-detail" href="/category/2nd_term">2기</a></div>
          </div>
      </div>
      <div class="detail-member">
        <div class="tab-item">
          <div><a class="page-link-detail" href="/members/1">1기</a></div>
          <div><a class="page-link-detail" href="/members/2">2기</a></div>
        </div>
    </div>
    </div>

  </div>

  <script type="text/javascript">
    var tabMenu = $("#tab");
    var tabSubMenu = $("#hovering");

    tabSubMenu.hide()

    tabMenu.hover(function() {
      tabSubMenu.show();
    }, function() {
      tabSubMenu.hide();
    })
  </script>

</header>


    <div class="page-content" id="page-content">
      <div class="wrapper" id="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">4주차 ML 지도학습 두번째 시간</h1>
    <h1 class="post-description"></h1>
    <p class="post-meta">October 6, 2021 — 00:00 • keonju2</p>
    
      
        <span class="tag">ml</span>
      
    
  </header>
  <article class="post-content">
    <h1 id="지난주-과제와-이번주-내용">지난주 과제와 이번주 내용</h1>
<p><br /></p>
<ul>
  <li>TIL 작성과 백준 10문제 풀기</li>
</ul>

<p><br />
<br />
이번주는 지도학습 2주차로 지도학습에서 남은 부분과 분류 예측의 불확실성 추정을 주제로 학습하였습니다.<br />
제가 담당한 부분은 지도부분 남은 부분 중에서 결정 트리, 앙상블 크게 두 부분으로 볼 수 있습니다.<br />
<br />
<br /></p>

<h1 id="결정-트리">결정 트리</h1>
<p><span style="color:orange">Decision Tree </span></p>

<p>결정 트리는 분류와 회귀 문제에서 사용되는 모델입니다.<br />
스무고개처럼 예/아니오로 나눌 수 있는 조건을 통해서 결정에 다다르게 됩니다.<br />
질문과 정답은 노드가 되고 특히 마지막 노드는 리프라고 합니다. 
<br /><br />
<img src="https://user-images.githubusercontent.com/54880474/136059620-b1adf458-14a4-474b-ae07-1fb89a48f29e.png" alt="tree1" /></p>

<p>결정트리의 구조는 왼쪽 하단에 사진처럼 가장 위에는 Root node, 질문과 답을 연결하는 Edge, 내부의 Internal node, 마지막 노드는 Leaf node, 그리고 Depth로 구성됩니다.</p>

<p><br /></p>

<hr />

<p><br /></p>

<h4 id="결정-트리-만들기-"><span style="color:green">결정 트리 만들기 </span></h4>
<p><br />
결정 트리를 학습한다는 것은 정답에 가장 빨리 도달하는 예/아니오 질문 (TEST) 목록을 학습한다는 뜻입니다.<br />
보통 데이터들은 예/아니오 특성으로 구분되지 않고  연속적인 특성을 가진 2차원 데이터 셋에서 보통 ‘특성 i는 값 a보다 큰가?’의 형태와 같은 테스트를 가집니다.<br />
<br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136060832-f157681f-43fa-4b0d-a159-b7554269124b.png" alt="tree2" /></p>

<p>이 데이터들을 X[1]&lt;=0.6인 테스트로 나누어 봅니다.</p>

<p><img src="https://user-images.githubusercontent.com/54880474/136061884-6c694665-0037-4ba9-9e82-25dd6d9d5c6a.png" alt="tree3" /></p>

<p>알고리즘은 가능한 모든 테스트에서 타깃 값에 대해 가장 많은 정보를 가진 것을 고르게 됩니다.</p>

<p>따라서 X[1]&lt;=0.6인 테스트를 선택하게 됩니다.<br />
<br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136062820-b9733fdf-be5b-4aaf-a6c4-02114c7cb748.png" alt="tree5" /></p>

<p>결정 트리에서 각 테스트는 하나의 축을 따라 데이터를 나눕니다.<br />
하나의 질문당 하나의 축을 만들어서 영역이 한 개의 타깃값을 가질 때까지 반복됩니다. <br />
<br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136062365-e168f4b1-9828-487a-bcd1-de32998eafb2.png" alt="tree4" /></p>

<p>결정 트리의 멈춤 조건입니다.<br />
즉, 미리 정의한 조건들이 없다면 가지를 만들 수 있을 때까지 만드는 것을 알 수 있습니다.<br />
결정트리의 예측은 그 포인트가 어느 리프에 들어갈지 확인하는 것인데 분류는 타깃 값 중 다수인 것이 예측 결과가 되고 회귀의 경우 리프 노드의 훈련 데이터 평균값이 결과로 출력됩니다.</p>

<p><br /></p>

<hr />

<p><br /></p>

<h4 id="결정-트리-복잡도-제어하기-"><span style="color:skyblue">결정 트리 복잡도 제어하기 </span></h4>
<p><br />
결정 경계가 클래스 포인트에 멀리 떨어진 이상치에 민감하게 되어 모든 리프 노드가 순수 노드가 될 때까지 진행하면 모델이 복잡해지고 과대적합이 발생합니다.<br />
과대적합을 막기 위한 방법은 크게 사전가지치기, 사후 가지치기 두 가지입니다.<br />
사전 가지치기는 이름에서 알 수 있듯이 모델을 만들 때 깊이나 리프의 개수 또는 테스트의 최소 개수를 미리 제한하는 것입니다.<br />
미리 제한하기 때문에 정말로 중요한 포인트를 분류하지않을 수 있습니다.<br />
사후 가지치기 역시 이름에서 알 수 있듯이 트리가 만들어진 뒤 포인트가 적은 노드를 삭제 혹은 병합하게 되는데 에러감소 프루닝, 룰 포스트 프루닝 같은 방법들이 있습니다.<br />
<br />
<br /></p>

<p><span class="evidence">참고</span><br />
에러감소 프루닝</p>
<blockquote>
  <p>모든 노드를 프루닝 대상으로 고려<br />
노드 제거 후 검증을 통해 제거 전, 후 정확도 비교<br />
제거 전보다 정확도가 낮아지기 전까지 반복</p>
</blockquote>

<p>룰 포스트 프루닝</p>
<blockquote>
  <p>의사결정 트리를 룰셋으로 변환 (룰은 루트부터 리프까지의 경로)<br />
이 룰셋 속성들에 정확도를 떨어뜨리는 속성을 제거<br />
프루닝 완료 후 정확도 순으로 정렬해 이 순서대로 적용</p>
</blockquote>

<p><br />
<br /></p>

<p>결정 트리는 다음과 같이 만들 수 있고 정확도를 확인할 수 있습니다.<br />
Default값은 모든 리프가 순수 노드가 되는 모델을 만들기 때문에 훈련 세트의 정확도가 100%가 됩니다.<br />
하지만 트리가 무한정 깊어지고 복잡해지고 일반화가 잘 되지 않습니다.
<br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">cancer</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">cancer</span><span class="p">.</span><span class="n">target</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">cancer</span><span class="p">.</span><span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tree</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">tree</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">tree</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>

<span class="n">훈련</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">1.000</span>
<span class="n">테스트</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.937</span>
</code></pre></div></div>
<p><br />
과대적합 때문에 반드시 훈련 세트의 정확도가 테스트 정확도와 비례하지 않아서 max_depth와 같은 파라미터를 통해 과대적합을 줄이고 테스트 세트 정확도를 높일 수 있습니다.</p>

<p><br />
<br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tree</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">tree</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">tree</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>

<span class="n">훈련</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.988</span>
<span class="n">테스트</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.951</span>
</code></pre></div></div>

<p><br /></p>

<hr />
<p><br /></p>

<h4 id="결정-트리-분석-"><span style="color:purple">결정 트리 분석 </span></h4>
<p><br /></p>

<p>결정 트리를 생성하고 시각화하기 위해서는 다음과 같은 모듈이 필요합니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 트리 모델 생성
</span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span> 

<span class="c1"># 트리의 시각화_1
</span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">export</span> <span class="n">graphviz</span> 

<span class="c1"># 트리의 시각화_2 (.dot 파일을 만들지 않아도 가능)
</span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">plot_tree</span> 
</code></pre></div></div>

<p><br />
그래프를 시각화하는 코드는 다음과 같이 쓸 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># graphviz 이용
</span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">export_graphviz</span>

<span class="n">export_graphviz</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">out_file</span><span class="o">=</span><span class="s">"tree.dot"</span><span class="p">,</span> <span class="n">class_names</span><span class="o">=</span><span class="p">[</span><span class="s">"악성"</span><span class="p">,</span> <span class="s">"양성"</span><span class="p">],</span>
                <span class="n">feature_names</span><span class="o">=</span><span class="n">cancer</span><span class="p">.</span><span class="n">feature_names</span><span class="p">,</span> <span class="n">impurity</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1">#plot_tree
</span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">plot_tree</span>

<span class="n">plot_tree</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">class_names</span><span class="o">=</span><span class="p">[</span><span class="s">"악성"</span><span class="p">,</span> <span class="s">"양성"</span><span class="p">],</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">cancer</span><span class="p">.</span><span class="n">feature_names</span><span class="p">,</span>
         <span class="n">impurity</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">rounded</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</code></pre></div></div>

<p>filled=True를 넣어주면 다음과 같이 색상이 들어가는 트리 모델을 얻을 수 있습니다.</p>

<p><img src="https://user-images.githubusercontent.com/54880474/136067667-33e3ca00-4752-4125-9d5e-ff12703b4d94.png" alt="tree8" /></p>

<p><br /></p>

<hr />

<p><br /></p>

<h4 id="트리의-특성-중요도"><span style="color:yellow">트리의 특성 중요도</span></h4>
<p><br /></p>

<p>tree.feature_importane를 통해 특성 중요도를 알 수 있습니다.<br />
특성 중요도는 0부터 1 사이에 존재하는데 0은 전혀 사용되지 않은 특성, 1은 완벽하게 타깃 클래스를 예측한 특성을 의미합니다. <br />
특성 중요도가 낮다는 유용하지 않다가 아닌 모델이 만들어질 때 특성을 선택하지 않았거나 특성과 중복되는 정보가 있다는 것을 의미합니다.<br />
전체 합은 1이 되고 따라서 특성중요도는 ‘이 모델이 만들어지는데 어떤 특성의 비율이 높은가?’ 정도의 해석이라고 생각하면 될 것 같습니다.<br />
Worst_radius만 보고 ‘반지름이 크면 양성이다?’ 를 알 수 없는 것처럼 특성 중요도는 어떤 클래스를 지지하는지 알려주지 않습니다.<br />
<br />
결정 트리의 회귀도 분류와 비슷하게 적용됩니다.
단, 결정 트리를 회귀 모델로 사용하게 되면 훈련 데이터 범위 밖의 정보가 없어서 그 부분에 대한 예측이 불가능하게 됩니다.</p>

<p><img src="https://user-images.githubusercontent.com/54880474/136069236-7fb89212-fcc1-4919-b9ae-430f2a18c359.png" alt="tree9" /></p>

<p>다음 모델은 트리 복잡도에 제한을 두지않아서 훈련 데이터는 완벽하게 예측하지만 데이터 범위 밖으로 나가면 마지막 포인트로 예측값을 출력합니다.
따라서 트리 모델은 가격의 등락과 같은 예측을 할 때는 좋은 예측 모델을 만들 수 있지만 시계열 데이터에서는 데이터가 가진 시간 범위 밖의 예측은 안되기 때문에 잘 맞지 않습니다.<br />
<br /></p>

<hr />
<p><br /></p>

<h4 id="장단점과-매개변수"><span style="color:pink">장단점과 매개변수</span></h4>
<p><br /></p>

<p>장점</p>
<blockquote>
  <p>해석력이 높습니다.<br />
데이터의 스케일에 구애받지 않습니다. <br />
정규화나 표준화 같은 전처리 불필요합니다.
특성의 스케일이 다르거나 이진특성, 연속적인 특성이 혼합되어도 잘 작동합니다.</p>
</blockquote>

<p>단점</p>
<blockquote>
  <p>과대적합되는 경향이 있어 일반화 성능이 좋지 않습니다.  <br />
축 평행을 구분하여 일부 관계에서 모델링이 어려움이 있습니다. <br />
훈련 데이터에 대한 약간의 변경은 전체 결정논리에 큰 변화를 야기하여 샘플에 민감합니다.</p>
</blockquote>

<p><br /></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">매개변수</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>min_samples_split</strong></td>
      <td>- 노드를 분할하기 위한 최소한의 샘플 데이터수 → 과적합을 제어하는데 사용 <br /> - Default = 2 → 작게 설정할 수록 분할 노드가 많아져 과적합 가능성 증가</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>min_samples_leaf</strong></td>
      <td>- 리프노드가 되기 위해 필요한 최소한의 샘플 데이터수<br />- min_samples_split과 함께 과적합 제어 용도<br />- 불균형 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 작게 설정 필요</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>max_features</strong></td>
      <td>- 최적의 분할을 위해 고려할 최대 feature 개수<br />- Default = None → 데이터 세트의 모든 피처를 사용<br />- int형으로 지정 →피처 갯수 / float형으로 지정 →비중<br />- sqrt 또는 auto : 전체 피처 중 √(피처개수) 만큼 선정<br />- log : 전체 피처 중 log2(전체 피처 개수) 만큼 선정</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>max_depth</strong></td>
      <td>- 트리의 최대 깊이<br />- default = None<br />→ 완벽하게 클래스 값이 결정될 때 까지 분할 또는 데이터 개수가 min_samples_split보다 작아질 때까지 분할<br />- 깊이가 깊어지면 과적합될 수 있으므로 적절히 제어 필요</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>max_leaf_nodes</strong></td>
      <td>리프노드의 최대 개수</td>
    </tr>
  </tbody>
</table>

<p>여기서 max_depth, max_leaf_nodes,min_samples_leaf 중 하나만 지정해도 과대적합을 막는데 충분한 역할을 합니다.</p>

<p><br />
<br />
<br /></p>

<h1 id="결정-트리의-앙상블">결정 트리의 앙상블</h1>
<p><span style="color:orange">Ensemble</span></p>

<p>앙상블은 여러 머신러닝 모델을 연결하여 더 강력한 모델을 만드는 기법입니다.<br />
책에서는 결정 트리의 앙상블로 한정하고 가장 많이 쓰이는 랜덤포레스트나 부스팅 모델은 트리 기반 모델이지만 앙상블은 다른 분류 모델을 결합하여 사용할 수도 있습니다.</p>

<ul>
  <li>Voting – 서로 다른 알고리즘을 가진 분류기를 결합</li>
  <li>Bagging – 각각의 분류기는 모두 같은 유형의 알고리즘 기반, 모델을 다양하게 만들기 위해 데이터를 재구성 (랜덤포레스트)</li>
  <li>Boosting – 맞추기 어려운 데이터에 대해 좀 더 가중치를 두어 학습 (Adaboost, Gradient Boosting)</li>
  <li>Stacking – 모델의 output 값을 새로운 독립변수로 사용<br />
<br /></li>
</ul>

<p><img src="https://user-images.githubusercontent.com/54880474/136071927-2b58ace4-f191-497f-b214-4077ff7aad64.png" alt="ensemble1" /></p>

<p>앙상블의 조건입니다.<br />
<br /></p>

<hr />

<p><br /></p>

<h4 id="랜덤-포레스트"><span style="color:lightgreen">랜덤 포레스트</span></h4>

<p><br /></p>

<p>랜덤 포레스트는 조금씩 다른 결정 트리의 묶음입니다.      <br />
데이터의 일부에 과대적합되는 경향을 이용하여 서로 다른 방향으로 과대적합된 트리를 많이 만들어 그 결과를 평균냄으로써 예측 성능은 유지되면서 결과적으론 과대적합이 줄어드는 아이디어에 기초합니다.<br />
결정 트리를 많이 만들면서 각 트리는 타깃 예측을 잘 해야 하고 다른 트리와 구별되어야 합니다.<br />
따라서 무작위성을 주입하는데 트리를 만들 때 사용하는 데이터 포인트를 무작위로 선택하거나 분할 테스트에서 특성을 무작위로 선택하는 방법을 이용합니다.<br />
<br /></p>

<hr />

<p><br /></p>

<h4 id="랜덤-포레스트-구축"><span style="color:olive">랜덤 포레스트 구축</span></h4>

<p><br /></p>

<p>from sklearn.ensemble import RandomForestClassifier (or RandomForestRegressor)<br />
n_estimators로 생성할 트리의 개수를 정합니다.<br />
부트스트랩 샘플은 n_samples개의 데이터 포인트 중에서 n_samples 횟수만큼 무작위로 중복 가능하게 반복 추출하는 것을 의미합니다.<br />
따라서 데이터 셋이 원래 크기와 같지만 누락되거나 중복되는 데이터가 만들어집니다.<br />
<br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136072938-20fe1a7b-c4e1-4a72-b7ac-08c4dbc16744.png" alt="ensemble2" /></p>

<p><br /></p>

<p>각 노드에서 전체 특성을 대상으로 최선의 테스트를 찾는 것이 아닌 알고리즘이 각 노드에서 후보 특성을 무작위로 선택한 후 이 후보들 중에서 최선의 테스트를 찾습니다. (max_features) <br />
부트스트랩 샘플링을 통해 트리가 조금씩 다른 데이터셋을 이용해 만들어지도록 합니다.<br />
각 노드에서 특성의 일부만 사용하기 때문에 트리의 각 분기는 각기 다른 특성 부분 집합을 사용됩니다.</p>

<p>max_features=n_features는 특성 선택에 무작위성이 들어가지 않습니다. (부트스트랩 샘플링에는 무작위성 그대로 입니다.)<br />
max_feature=1 트리의 분기는 테스트할 특성을 고를 필요가 없게 되고 무작위로 선택한 특성의 임계값 찾기만 하면 됩니다.<br />
max_feature이 커지면 랜덤 포레스트 트리들은 매우 비슷하고 가장 두드러진 특성으로 데이터에 잘 맞춰질 것이고 작으면 트리들은 서로 많이 달라지고 각 트리는 데이터에 맞추기 위해 깊이가 깊어지게 됩니다.</p>

<p>랜덤 포레스트 예측의 경우 알고리즘이 모델에 있는 모든 트리의 예측을 만듭니다.<br />
회귀의 경우 이 예측들을 평균하여 최종 예측을 만듭니다.<br />
분류의 경우 약한 투표 전략을 사용합니다.<br />
약한 투표 전략은 각 알고리즘이 가능성 있는 출력 레이블의 확률을 제공하고 예측한 확률을 평균으로 가장 높은 확률을 가진 클래스가 예측값이 됩니다.<br />
참고로 강한 투표 전략은 다수의 분류기가 결정한 예측값을 최대로 하는 것을 말합니다.</p>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136073413-36133d42-fdab-4036-a310-3ed03cac37be.png" alt="ensemble3" /></p>

<p><br /></p>

<hr />

<p><br /></p>

<h4 id="랜덤-포레스트-분석"><span style="color:darkblue">랜덤 포레스트 분석</span></h4>

<p><br /></p>

<p>랜덤 포레스트 훈련 모델</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">forest</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">forest</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/54880474/136073874-0e16312c-47e8-4567-b4a4-545001f1d605.png" alt="ensemble4" /></p>

<p>부트스트랩 샘플링 때문에 한쪽 트리에 나타나는 훈련 포인트가 다른 트리에는 포함되지 않을 수 있어 각 트리는 불완전하지만 랜덤포레스트의 결과는 좋은 결정경계를 보여줍니다.</p>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136074021-efc81063-ca11-437d-83f1-0c8b171a01e6.png" alt="ensemble5" /></p>

<p>단일 트리와 다르게 랜덤 포레스트에서 가장 특성 중요도가 높은 특성은 worst perimeter입니다.<br />
랜덤 포레스트에서 더 많은 특성이 0 이상의 중요도를 갖고 따라서 더 넓은 시각으로 데이터를 바라볼 수 있습니다.</p>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">cancer</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">cancer</span><span class="p">.</span><span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">forest</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">forest</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">forest</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">forest</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>

<span class="n">훈련</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">1.000</span>
<span class="n">테스트</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.972</span>
</code></pre></div></div>

<p><br /></p>

<p>랜덤 포레스트에선 훈련 데이터 정확도가 100% 이지만 단일 트리에 비해서 테스트 정확도가 상승한 것을 확인 할 수 있습니다.</p>

<p><br /></p>

<hr />

<p><br /></p>

<h4 id="장단점과-매개변수-1"><span style="color:chocolate">장단점과 매개변수</span></h4>

<p><br /></p>

<p>장점</p>
<blockquote>
  <p>매개변수 튜닝을 많이 하지 않습니다. <br />
데이터의 스케일에 구애받지 않습니다. <br />
단일 트리의 단점을 보완하고 장점을 그대로 가지고 있습니다.</p>
</blockquote>

<p>단점</p>
<blockquote>
  <p>랜덤 포레스트의 트리는 특성의 일부만 사용하므로 결정 트리보다 더 깊어지는 경향이 있습니다.<br />
다른 random_state를 지정하면 전혀 다른 모델이 만들어집니다.<br />
텍스트 데이터와 같은 차원이 높고 희소한 데이터에 잘 작동하지 않습니다.<br />
선형 모델에 비해 많은 메모리를 사용하며 훈련과 예측이 느림</p>
</blockquote>

<p><br /></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">매개변수</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>n_estimators</strong></td>
      <td>- 결정트리의 갯수를 지정<br />- Default = 10 (0.22버전부터 100)<br />- 무작정 트리 갯수를 늘리면 성능 좋아지는 것 대비 시간이 걸릴 수 있음</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>min_samples_split</strong></td>
      <td>- 노드를 분할하기 위한 최소한의 샘플 데이터수 → 과적합을 제어하는데 사용<br />- Default = 2 → 작게 설정할 수록 분할 노드가 많아져 과적합 가능성 증가</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>min_samples_leaf</strong></td>
      <td>- 리프노드가 되기 위해 필요한 최소한의 샘플 데이터수<br />- min_samples_split과 함께 과적합 제어 용도<br />- 불균형 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 작게 설정 필요</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>max_features</strong></td>
      <td>- 최적의 분할을 위해 고려할 최대 feature 개수<br />- Default = ‘auto’ (결정트리에서는 default가 none이었음)<br />- int형으로 지정 →피처 갯수 / float형으로 지정 →비중<br />- sqrt 또는 auto : 전체 피처 중 √(피처개수) 만큼 선정 (RandomForestClassifier-sqrt(n_feature), RandomForestRegressor-n_feature)<br />- log : 전체 피처 중 log2(전체 피처 개수) 만큼 선정</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>max_depth</strong></td>
      <td>- 트리의 최대 깊이<br />- default = None<br />→ 완벽하게 클래스 값이 결정될 때 까지 분할 또는 데이터 개수가 min_samples_split보다 작아질 때까지 분할<br />- 깊이가 깊어지면 과적합될 수 있으므로 적절히 제어 필요</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>max_leaf_nodes</strong></td>
      <td>리프노드의 최대 개수</td>
    </tr>
  </tbody>
</table>

<p>N_estimatiors는 클수록 좋고 max_features와 max_depth와 같은 사전 가지치기 옵션은 단일 트리와 같이 주어집니다.</p>

<p><br /></p>

<hr />

<p><br /></p>

<h4 id="그레이디언트-부스팅-회귀-트리"><span style="color:fuchsia">그레이디언트 부스팅 회귀 트리</span></h4>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136076700-214be147-9549-47c5-9681-f20564458f85.png" alt="gradient" /></p>

<p><br /></p>

<p>이름은 회귀이지만 회귀와 분류 모두 사용됩니다. (GradientBoostingClassifier, GradientBoostingRegressor)<br />
그레이디언트 부스팅은 이전 트리의 오차를 보완하는 방식으로 순차적으로 트리를 만듭니다.<br />
따라서 기본적으로 무작위성이 없습니다.<br />
대신 강력한 사전 가지치기가 사용되고 깊지 않은 트리를 사용합니다.<br />
각 트리는 데이터의 일부에 대해서만 예측을 잘 수행하여 트리가 많이 추가될수록 성능이 향상됩니다.<br />
이때 손실 함수를 정의하고 경사 하강법을 사용해서 다음 값을 보정합니다.<br />
<br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136076846-03cd6618-ee17-4afd-869f-09ed4543156f.png" alt="gradient1" /></p>

<p><br />
<br /></p>

<p>random_state=0 만 입력했을 때</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
<span class="err">​</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">cancer</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">cancer</span><span class="p">.</span><span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="err">​</span>
<span class="n">gbrt</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">gbrt</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="err">​</span>
<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gbrt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gbrt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>

<span class="n">훈련</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">1.000</span>
<span class="n">테스트</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.965</span>
</code></pre></div></div>
<p><br /></p>

<p>random_state=0, max_depth=1 을 입력했을 때</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gbrt</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">gbrt</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="err">​</span>
<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gbrt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gbrt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>

<span class="n">훈련</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.991</span>
<span class="n">테스트</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.972</span>
</code></pre></div></div>
<p><br /></p>

<p>random_state=0, learning_rate=0.01을 입력했을 때</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gbrt</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">gbrt</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gbrt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gbrt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>

<span class="n">훈련</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.988</span>
<span class="n">테스트</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.965</span>
</code></pre></div></div>
<p><br /></p>

<p>훈련 세트의 정확도가 100%로 과대적합이 된 모델은 max_depth나 learning_rate로 보완할 수 있습니다.<br />
Random_state는 고정시켜야 같은 모델이 나오는 것을 볼 수 있습니다.<br />
Learning_rate는 오차에 곱을 해서 예측값을 업데이트 해주는 값입니다.<br />
<br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136077751-6aba3074-c68d-4789-a9af-3d657f671ab2.png" alt="gradient2" /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136077761-e5e43c6c-e62a-4c10-a7e6-96e584cec03e.png" alt="gradient3" /></p>

<p>랜덤 포레스트에 비해 그레이디언트 부스팅은 특성들이 더 적습니다.<br />
안정성에서는 랜덤 포레스트가 더 좋지만 그레이디언트가 성능적으로 더 좋은 모습을 보여줄수 있습니다.<br />
<br />
<br /></p>

<p><span class="evidence">참고</span><br />
XGBoost</p>
<blockquote>
  <p>XGBoost는 데이터 별 오류를 다음 round 학습에 반영 시킨다는 측면에서 기존 Gradient Boosting과 큰 차이는 없음<br />
Gradient Boosting과 달리 학습을 위한 목적식(loss function)에 Regularization term이 축가되어 모델이 과적합 되는 것을 방지해줌<br />
Regularization term을 통해 XGBoost는 복잡한 모델에 패널티를 부여함</p>
</blockquote>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136078326-964b2d5e-2e10-47a7-9300-e2cd383c810c.png" alt="gradient4" /></p>

<p><br /></p>

<p>LighGBM</p>
<blockquote>
  <p>XGBoost와 다르게 lear-wise loss 사용 (loss를 더 줄일 수 있음)<br />
XGBoost 대비 2배 이상 빠른 속도 (동일 파라미터 기준)<br />
과대적합에 민감하여, 대량의 학습데이터를 필요로 함</p>
</blockquote>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136078359-4d3bf782-bcbe-4e5d-b101-b5ccae3808f4.png" alt="gradient5" /></p>

<p><br /></p>

<hr />

<p><br /></p>

<h4 id="장단점과-매개변수-2"><span style="color:teal">장단점과 매개변수</span></h4>

<p><br /></p>

<p>장점</p>
<blockquote>
  <p>이진 특성이나 연속적인 특성에도 잘 작동합니다. <br />
데이터의 스케일에 구애받지 않습니다.</p>
</blockquote>

<p>단점</p>
<blockquote>
  <p>매개변수의 조정이 필수입니다.<br />
휸련시간이 깁니다.  <br />
차원이 높고 희소한 데이터에 잘 작동하지 않습니다.</p>
</blockquote>

<p>N_estimators가 클수록 랜덤 포레스트는 좋았지만 그래이디언트 부스팅에서는 과대적합될 가능성이 높아집니다.<br />
N_estimator을 정하고 난 뒤에 learning_rate를 정하게 되는데 learning_rate를 낮추면 비슷한 복잡도의 모델을 만들기 위해 더 많은 트리를 추가해야합니다.</p>

<p><br /></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">매개변수</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>n_estimators</strong></td>
      <td>- 트리의 개수를 지정<br />- 커지면 모델이 복잡해지고 과대적합 가능성 높아짐</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>learning_rate</strong></td>
      <td>- 관례상 n_estimators를 맞추고 learning_rate를 찾음<br /> - 이전 트리의 오차를 보정하는 정도</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>n_iter_no_change / validation_fraction</strong></td>
      <td>-조기 종료를 위한 매개변수 (default값: n_iter_no_change =None (조기 종료 x), validation_fraction=0.1) <br />- validation_fraction 비율만큼 검증 데이터로 사용하여 n_iter_no_change 만큼 반복하여 향상되지 않으면 훈련 종료</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>max_depth / max_leaf_nodes</strong></td>
      <td>-각 트리의 복잡도를 낮춤 <br />- max_depth는 보통 매우 작게 설정하며 트리의 깊이가 5보다 깊어지지 않게 함</td>
    </tr>
  </tbody>
</table>

<p><br />
<br />
<br /></p>

<h1 id="배깅-엑스트라-트리-에이다부스트">배깅, 엑스트라 트리, 에이다부스트</h1>
<p><span style="color:maroon">Bagging</span></p>

<p>from sklearn.ensemble import BaggingClassifier<br />
배깅은 중복을 허용한 랜덤샘플링으로 만든 훈련 세트를 사용해 분류기를 각기 다르게 학습합니다.<br />
랜덤포레스트는 배깅의 일종이지만 설명변수도 무작위로 선택하는 것이 차이가 있습니다.<br />
predict_proba() 지원하면 메서드를 통해 확률값을 평균하여 예측을 수행합니다. (지원하지 않는다면 가장 빈도가 높은 클래스 레이블)<br />
oob_score=True로 지정하면 매개변수는 부트스트래핑에 포함되지 않은 샘플로 훈련된 모델을 평가할 수 있습니다. (OOB 오차, default=False)<br />
<br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span>
<span class="n">bagging</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(),</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">oob_score</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">bagging</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xc_train</span><span class="p">,</span> <span class="n">yc_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">bagging</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xc_train</span><span class="p">,</span> <span class="n">yc_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">bagging</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xc_test</span><span class="p">,</span> <span class="n">yc_test</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"OOB 샘플의 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">bagging</span><span class="p">.</span><span class="n">oob_score_</span><span class="p">))</span>

<span class="n">훈련</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.953</span>
<span class="n">테스트</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.951</span>
<span class="n">OOB</span> <span class="n">샘플의</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.946</span>
</code></pre></div></div>
<p><br /></p>

<p>배깅은 랜덤포레스트와 달리 max_samples에 부트스트랩 샘플의 크기를 정할 수 있습니다. <br />
또한 로지스틱 회귀가 들어갈 수도 있고 결정 트리가 들어갈 수도 있습니다.</p>

<p><br /></p>

<hr />
<p><br /></p>

<p><span style="color:lime">Extra Tree</span></p>

<p>후보 특성을 무작위로 분할한 다음 최적의 분할을 찾습니다.<br />
엑스트라 트리도 랜덤 포레스트와 비슷하지만 splitter=‘random’을 사용합니다. 랜덤 포레스트는 splitter=‘best’가 고정입니다.<br />
Splitter=‘best’의 의미는 모든 변수의 정보 이득을 계산하고 그중 가장 설명력이 높은 변수를 선택하는 것입니다.<br />
또한 부트스트랩 샘플링을 적용하지 않습니다. <br />
무작위성을 증가시키면 모델 편향은 늘어나지만 분산이 감소하는 모습을 보입니다.<br />
개별 트리는 매우 복잡하지만 결정 경계는 안정적입니다.<br />
계산 비용은 위 splitter에서의 feature의 차이 때문에 랜덤 포레스트보다 적지만  일반화 성능을 높이려면 많은 트리를 만들어야합니다.<br />
<br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">ExtraTreesClassifier</span>
<span class="n">xtree</span> <span class="o">=</span> <span class="n">ExtraTreesClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">xtree</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xc_train</span><span class="p">,</span> <span class="n">yc_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">xtree</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xc_train</span><span class="p">,</span> <span class="n">yc_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">xtree</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xc_test</span><span class="p">,</span> <span class="n">yc_test</span><span class="p">))</span>

<span class="n">훈련</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">1.000</span>
<span class="n">테스트</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.972</span>
</code></pre></div></div>

<p><br /></p>

<hr />
<p><br /></p>

<p><span style="color:gray">Adaptive Boosting</span></p>

<p>이전의 모델이 잘못 분류한 샘플에 가중치를 높여서 다음 모델을 훈련합니다.<br />
훈련된 각 모델은 성능에 따라 가중치 부여합니다.<br />
예측을 만들 때는 모델이 예측한 레이블을 기준으로 모델의 가중치를 합산하여 가장 높은 값을 가진 레이블을 선택합니다.<br />
AdaBoostClassifier은 기본값으로 DecisionTreeClassifier(max_depth=1)를 갖습니다.<br />
AdaBoostRegressor은 기본값으로 DecisionTreeRegressor(max_depth=3)을 갖습니다. (base_estimator을 이용하여 다른 모델 지정 가능)</p>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136081550-a82cd75f-c6fd-49d2-83e9-8fedd01c4d5e.png" alt="ada1" /></p>

<p>에이다 부스팅의 원리와 수식
<br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136081678-0d0819ea-b048-43f2-a786-c470918bd8d1.png" alt="ada2" /></p>

<p><br /></p>

<p>예측정확도와 가중치의 곱의 합이 되어 높은 정확도를 만들게 됩니다.<br />
<br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>
<span class="n">ada</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">ada</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xc_train</span><span class="p">,</span> <span class="n">yc_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">ada</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xc_train</span><span class="p">,</span> <span class="n">yc_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">ada</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xc_test</span><span class="p">,</span> <span class="n">yc_test</span><span class="p">)))</span>

<span class="n">훈련</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">1.000</span>
<span class="n">테스트</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.986</span>
</code></pre></div></div>
<p><br />
<br /></p>

<h1 id="마무리">마무리</h1>

<p>사실 결정 트리를 배우기 위해서 엔트로피, 정보 이득, 지니 계수와 같은 개념들도 알아두는 것이 좋습니다.</p>

<p>앙상블 단계에서도 보팅, 배깅, 부스팅, 스택킹에 대한 개념과 실제 사용되는 모습에 대해 공부하였으면 좋았을 것 같습니다.<br />
각 모델마다 어떤 아이디어에서 발생했는지 알게 된다면 좀 더 이해가 쉬웠을 것 같습니다.</p>

<p>하드 보팅과 소프트 보팅의 개념과 원리에 대해서도 공부가 필요했습니다.</p>

<p>책에서는 수학적 개념이나 머신러닝 이론이 자세하게 설명되어 있지는 않았습니다.<br />
하지만 이론적인 부분을 다룬 내용들은 구글에 검색하면 쉽게 찾을 수 있으니까 따로 공부하면 좋을 것 같습니다.</p>

<p>정리만 하는 건데 이렇게 길어질 줄은 몰랐습니다…ㅠ<br />
이게 4주차 절반 분량이었는데 분량 조절 실패해서 죄송합니다. ㅠㅠ</p>

  </article>
  <br>

  <hr/><br>

  <div class="author">
    
    <table>
        <tr>
          <td rowspan="3" style="padding-right: 10px"><img class="author-pic" src="https://github.com/.png" alt=""></td>
          <td><b><h2 rel="author"></h2></b></td>
        </tr>
      <tr>
        <td><p rel="author"></p>
        </td>
      </tr>
      <tr>
        <td>
          <a rel="author" href="https://github.com/" target="_blank"><i class="fa fa-github fa-2x"></i></a>&nbsp&nbsp
          
        </td>
      </tr>
    </table>

  </div>


  <br>
  <hr/>
  <div>
    <script src="https://utteranc.es/client.js"
        repo="gdsc-seoultech/blog-comments"
        issue-term="pathname"
        label="comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
    </script>
  </div>

</div>
<script src="/js/toc.js"></script>
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper" align="center">
  	<h4>This site was built using <a href="http://jekyllrb.com" target="_blank">Jekyll</a> and is hosted on <a href="https://github.com" target="_blank">Github</a>. &#169; GDSC Seoultech</h4>
  </div>

</footer>


  </body>
</html>
