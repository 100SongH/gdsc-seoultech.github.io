---
layout: post
title: MediMedi 3주차 보고서
date: 2022-01-14
author: leeeha
description: 
categories: ["medimedi"]
---

안녕하세요 여러분! 저는 솔루션 챌린지 MediMedi 팀에서 안드로이드를 담당하고 있는 이하은입니다! 블로그에 정말 오랜만에 글을 써보네요! 현재 저희 팀의 진행 상황을 보고해보겠습니다! <br>

우선 저희는 시각 장애인분들에게 더욱 초점을 맞춰서 앱을 개발하기로 결정했습니다! 지난 주에는 사용자가 등록한 시간에 복약 알림을 주거나, 근처 약국을 검색할 수 있는 지도 서비스도 제공하려고 했으나, 이런 기능들은 이미 기존에 일반인들을 대상으로 많이 나와 있고 시각 장애인분들이 사용하기에는 불편함이 있을 것이라 판단했습니다! 그래서 이런 기능들은 일단 배제하고, **카메라로 약 상자의 텍스트를 인식해서 음성으로 약에 대한 정보를 알려주는 것부터** 먼저 구현해보기로 결정했습니다! <br>

UI도 시각 장애인분들이 사용하기 편하도록 보다 직관적이고 단순한 구조로 만들 생각입니다! 그리고 시각 장애라고 하면 보통 앞을 전혀 볼 수 없는 '전맹'을 가장 많이 생각하지만, 실제로 시각 장애인의 90%는 '저시력 장애'라고 합니다. [저시력 장애](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=kead1&logNo=220736968451)에는 시력저하, 시야장애, 적록색맹, 그리고 전색맹 등의 유형이 있습니다. 이런 점을 고려하여 콘텐츠와 배경이 명확히 구분되도록 명도 대비를 기준에 맞게 적용하고, 화면 확대나 고대비 기능 추가 등을 고려하고 있습니다. 아니면 버튼 자체를 크게 만들거나 [진동으로 신호를 주는](https://m.blog.naver.com/smart-touch/221587081637) 등의 방식도 고민하고 있습니다. '다수를' 위한 기술보다는 '모두를' 위한 기술을 만드는 데 이 앱이 조금이나마 보탬이 되었으면 좋겠네요! <br>

지난 주에는 '알약 자체'를 카메라로 찍을 생각을 했는데, [이 유튜브 영상](https://youtu.be/4d1eLBLyW9M)을 통해 시각 장애인 분들은 일반 상비약을 먹을 때도 점자 표기가 제대로 안 되어 있어서 마음 놓고 약을 먹기가 힘들다는 것을 알게 되었습니다. 그래서 '약 상자'를 카메라로 찍었을 때 약의 이름, 성분, 종류 중에 하나라도 제대로 추출이 되면, 그에 해당하는 약 정보를 음성으로 제공하기로 결정했습니다! <br>

# 이번 주 할일

그래서 이번 주에 할일은 다음과 같이 정했습니다! <br>

- 다같이: 메인 기능을 제외하고, 시각 장애인분들에게 더 도움이 될 만한 기능이 있는지 생각해보기 (관련 자료는 바로 바로 슬랙에 공유!)

- Android 윤재: 시각 장애인분들의 앱 접근성을 향상시키기 위한 UI 레퍼런스 조사하기

- Android 하은: 시각 장애인분들이 카메라의 위치 조정을 편하게 할 수 있도록, 음성으로 안내할 수 있는지 알아보기 (OpenCV를 이용해서 약 상자의 네 모서리가 모두 감지되면 카메라로 즉시 촬영하고, 그렇지 않으면 위치를 다시 조정해달라고 음성으로 알려주기, 다음 주에는 TTS api 사용법 알아보기)

- ML 민찬: 이미지에서 텍스트를 추출하는 OCR 기술을 앱에 어떻게 적용할지 알아보기, 검색에 사용될 약 데이터의 범위 정하기 (개수, 종류)

- Backend 수빈: 파이썬에서 OCR로 추출한 텍스트를 디비에서 어떻게 검색할지 알아보기

<br>

# 프로젝트에 사용되는 기술에 대하여

저희가 사용할 OpenCV, TTS, OCR 기술이 무엇인지 간략히 설명해보겠습니다! <br>

OpenCV는 Open Source Computer Vision의 약자로 다양한 이미지, 영상 처리에 사용할 수 있는 오픈소스 라이브러리입니다. 저는 이것을 이용하여 **안드로이드 카메라에서 약 상자의 네 모서리가 모두 감지되었을 때에만 즉시 사진을 촬영하는 것을 목표로** 하고 있습니다. 네 모서리가 제대로 감지되지 않았을 때는 "카메라 위치를 다시 조정해주세요"라고 음성으로 안내할 예정입니다. <br>

Warp Perspective <br>

<img src="https://user-images.githubusercontent.com/68090939/149617955-c228db7c-c0cd-4e62-844c-a98ed8631fff.png" width="400" height="300">

<br>

Contour Detection <br>

<img src="https://user-images.githubusercontent.com/68090939/149617999-2dbb38b1-d311-40af-8ffb-fda62f6d0de3.png" width="400" height="300">

<br>

TTS (Text-To-Speech)은 말그대로 텍스트를 음성으로 변환하는 음성 합성 기술입니다. 약 상자에서 추출한 텍스트를 디비에서 검색하고, 그 결과를 음성으로 알려주는 것을 목표로 하고 있습니다. 카메라에서 위치 조정을 할 때도 음성으로 안내를 해줘야 합니다. <br>

OCR (Optical Character Recognition, 광학 문자 인식)은 이미지(사진) 속 글자의 위치를 찾고 어떤 글자인지 자동으로 알아내는 기술입니다. 저희는 촬영된 이미지에서 약의 이름, 성분, 종류 등을 텍스트 형태로 추출하는 것을 목표로 하고 있습니다. <br>

<img src="https://user-images.githubusercontent.com/68090939/149619083-dc8d128f-2b0b-4ce7-bc89-95e12fdad4d0.png" width="500" height="400">

<br>

# 마무리

이렇게 규모 있는 프로젝트가 처음이고 OpenCV, TTS, OCR 모두 처음 사용해보는 기술들이라, 메인 기능을 잘 구현할 수 있을지 조금 걱정이 됩니다! 하지만 포기하지 않고 끝까지 해보겠습니다!!! 저희 팀 파이팅!!! <br>

원래 지난 주에 주요 기능을 다 정하고 UI와 와이어프레임을 짰어야 했는데, 이번 주에 타겟 유저층을 시각 장애인분들로 한정 지으면서 원래 계획의 많은 부분이 바뀌었습니다. 그래서 **다음 주에는 메인 기능 외에 시각 장애인분들께 도움이 될 만한 기능들을 몇 가지 더 정하고, UI와 와이어프레임을 짤 예정입니다.** <br>

저희 팀은 무엇보다도 **안드 + 백엔드 + ML이 조화를 이뤄서 메인 기능을 제대로 구현하는 것이 핵심**일 것이라 예상합니다. 그래서 이번 주뿐만 아니라 앞으로도 계속 새로 알게 된 기술 분야에 대해 공부하고, 이것을 저희의 앱에 어떻게 활용할 것인지 공부해나갈 것입니다! 저희가 원하는 기능을 모두 구현하는 그 날까지 파이팅입니다! <br>

